---
title: "Crowdsourcing surveys"
rct_id: "AEARCTR-0010095"
rct_id_num: "10095"
doi: "10.1257/rct.10095-1.0"
date: "2022-10-19"
status: "in_development"
jel: "D83, D84, D82, B41, D90, O31"
start_year: "2022-10-19"
end_year: "2022-11-07"
pi: "Mikhail Galashin"
pi_other:
abstract: "Researchers often ask members of the target population which evidence to collect. This includes asking which treatment arms to test, which questions to ask, and which response options to offer. Moreover, there is often a well defined goal that the researcher would want to optimize, such as the treatment effect, the predictive power of the question for some deep variable of interest, or the probability that the suggested survey options would be sufficient to classify all the respondents. However, there is little systematic evidence of the effects of such elicitation on the quality of the research design. We propose an experimental design to study this question: we first elicit suggestions for the best design of the evidence and then run a small version of the study for each of the elicited designs. This allows us to estimate the effect of monetary incentives and respondent characteristics on the design quality. 
"
layout: registration
---

